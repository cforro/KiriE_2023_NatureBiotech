{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pV48sOCihR1o",
    "outputId": "da286e9c-6b0c-4216-eedc-1a2ef6bfb842",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stumpy in /home/csaba/anaconda3/lib/python3.11/site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/csaba/anaconda3/lib/python3.11/site-packages (from stumpy) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5 in /home/csaba/anaconda3/lib/python3.11/site-packages (from stumpy) (1.10.1)\n",
      "Requirement already satisfied: numba>=0.54 in /home/csaba/anaconda3/lib/python3.11/site-packages (from stumpy) (0.57.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /home/csaba/anaconda3/lib/python3.11/site-packages (from numba>=0.54->stumpy) (0.40.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csaba/anaconda3/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/csaba/anaconda3/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/csaba/anaconda3/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy==1.19\n",
    "\n",
    "import os\n",
    "import fnmatch\n",
    "from scipy.signal import butter, filtfilt, fftconvolve,find_peaks\n",
    "#import numpy as np\n",
    "!pip install stumpy\n",
    "import stumpy\n",
    "import umap\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=3):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=3):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "def sliding_std(filtered_dat,sliding_win_size=20000):\n",
    "    #sliding window size (in datapoints) is the measurement unit for local std\n",
    "    #spikes are detected above 5std (neg, and pos)\n",
    "    #we have spurious discharges (a few datapoint-wide spikes, width being at half maximum.)\n",
    "    #we set minimum width to throw out spurious spikes, such as visible at the last quarter of channel [7]\n",
    "    #in baseline1.\n",
    "\n",
    "    kernel = np.ones( (sliding_win_size,1 ))/sliding_win_size #average counter\n",
    "    sig=filtered_dat.T\n",
    "    sliding_means = fftconvolve(sig,kernel,mode='same')\n",
    "    sig_ = sig-sliding_means\n",
    "    sliding_stds = np.sqrt( fftconvolve(sig_*sig_, kernel, mode='same'))\n",
    "    return sliding_stds\n",
    "def get_peaks(filtered_dat,sliding_stds):\n",
    "    peaks=[]\n",
    "    for i in range(len(filtered_dat)):\n",
    "        p,_=find_peaks( np.abs(filtered_dat[i]),height=5*sliding_std[:,i],width=peakwidth,rel_height=0.3)\n",
    "        peaks.append(p)\n",
    "    return peaks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-scQFhSIhR1s"
   },
   "source": [
    "## The Analyser class below helps create a more compact notebook, especially when a lot of data is being handled.\n",
    "\n",
    "### The analyser expects the data in the form of a numpy array. You can load your recording file (.rhs, etc), and save it with numpy into an .npy array, in the dimensions of (channel,samples)The current analyser was custom fit to our needs, please note that:\n",
    "\n",
    "* The data is sampled at 20kHz. It's bandpass between 210 and 9500 Hz. (3rd order acausal butterworth)\n",
    "* The standard deviation is computed in 1s (20000 samples) window, see definition in cell above\n",
    "* First, events larger than 5 local (within 1s) standard deviations are collected and clustered\n",
    "* * Then, with template matching, smaller events are fished out\n",
    "* Non-electrophysiological clusters are discarded optically, althoug heuristics could be built to automate this (peak half-width ratios, etc)\n",
    "* Extracted peaks for template matching and clustering consist of 30 samples before and 30 samples after the peak maximum.\n",
    "* Clustering is done via 2-component UMAP (from Umap-learn package) and density based clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMGs61jshR1u"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Analyser:\n",
    "    def __init__(self,path):\n",
    "        self.path = path\n",
    "        print('Loading data')\n",
    "        self.data = np.load(path+'/raw.npy')\n",
    "        print('Data loaded')\n",
    "        print('Filtering and getting windowed std')\n",
    "        self.filtdata=butter_bandpass_filter(self.data,lowcut=210,highcut=9500,fs=20000)\n",
    "        self.std=sliding_std(self.filtdata)\n",
    "        print('Done')\n",
    "        #self.get_peaks_and_get_waveforms()\n",
    "        self.is_clust=False\n",
    "    def get_peaks_and_get_waveforms(self):\n",
    "        all_chan_peaks=[]\n",
    "        for idx in range(len(self.filtdata)):\n",
    "            p,_=find_peaks( np.abs(self.filtdata[idx]),height=5*self.std[:,idx], distance=60)\n",
    "            all_chan_peaks.append(np.vstack((np.ones(len(p))*idx,p)).T)\n",
    "        self.all_chan_peaks=np.concatenate(all_chan_peaks).astype(int)\n",
    "        snips=[]\n",
    "        for chan,pix in self.all_chan_peaks:\n",
    "            snips.append(self.filtdata[chan][pix-30:pix+30])\n",
    "        snips=[el for el in snips if len(el)==60]\n",
    "        self.snips=np.asarray(snips)\n",
    "    def extract_shape_clusters(self,show_clusters=True,close_plots=False):\n",
    "        if not self.is_clust:\n",
    "            reducer=umap.UMAP(n_components=2)\n",
    "            print('Dimensionality reduction on detected waveforms')\n",
    "            projections=reducer.fit_transform(self.snips)\n",
    "            self.clust=DBSCAN(eps=0.5)\n",
    "            self.clust.fit(projections)\n",
    "            self.is_clust=True\n",
    "        if show_clusters:\n",
    "            n_clusters=np.max(self.clust.labels_)+1\n",
    "            labs=self.clust.labels_\n",
    "\n",
    "            fig,ax=plt.subplots(1,n_clusters, figsize=(n_clusters*3,3),sharex=True)\n",
    "            for i in range(n_clusters):\n",
    "                avg=np.median(self.snips[labs==i],axis=0)\n",
    "                std=np.std(self.snips[labs==i],axis=0)\n",
    "                l=len(self.snips[0])\n",
    "                x=np.linspace(-l//2,l//2,l)/20 #ms\n",
    "                ax[i].fill_between(x,y1=avg-std,y2=avg+std,alpha=0.3,color='b')\n",
    "                ax[i].plot(x,avg,color='r',linewidth=0.5)\n",
    "                ax[i].set_title('Cluster '+str(i)+', '+str(np.round(100*np.sum(labs==i)/len(labs),1))+'%')\n",
    "            ax[0].set_ylabel('Voltage [uV]')\n",
    "            ax[0].set_xlabel('Time [ms]')\n",
    "            plt.savefig(self.path+'/clusters.svg',bbox_inches='tight',pad_inches=0)\n",
    "        if close_plots:\n",
    "            plt.close()\n",
    "    def match_cluster(self,cluster_indices):\n",
    "        #has to be a list\n",
    "        channel_spindx=[]\n",
    "        for c in cluster_indices:\n",
    "            waveform=np.median(self.snips[self.clust.labels_==c],axis=0)[10:50]\n",
    "            wave_length=len(waveform)\n",
    "            seq=pd.DataFrame({'x': waveform})\n",
    "            match=np.asarray([stumpy.mass(seq['x'],pd.DataFrame({'x': filt})['x']) for filt in self.filtdata])\n",
    "            threshold=0.3*(2*np.sqrt(wave_length)) #scale-free is 0.3. Compensate by sequence length\n",
    "            #do spike detection to not double count !\n",
    "            #channel_spindx.append(np.asarray(np.where(match<threshold)).T)\n",
    "            for chan in range(len(match)):\n",
    "                p,_=find_peaks(-match[chan]+threshold,height=0,distance=60)\n",
    "                channel_spindx.append(np.vstack((chan*np.ones(len(p)),p)).T)\n",
    "        self.channel_spindx=np.concatenate(channel_spindx).astype(int)\n",
    "        np.save(self.path+'/channel_spindx.npy',self.channel_spindx)\n",
    "    def save_spike_cutouts(self):\n",
    "        self.spikes_dict={}\n",
    "        for chan in np.unique(self.channel_spindx[:,0]):\n",
    "            ch_sp=self.channel_spindx\n",
    "            a_chan=ch_sp[ch_sp[:,0]==chan].astype(int)\n",
    "            sigs=[self.filtdata[el[0],el[1]-5:el[1]+55] for el in a_chan]\n",
    "            self.spikes_dict[str( int(chan))]=sigs\n",
    "        with open(self.path+'/spike_shapes.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.spikes_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def produce_sample_detections(self,n_samples,window_size):\n",
    "        if not os.path.exists(self.path+'/sample_traces/'):\n",
    "            os.mkdir(self.path+'/sample_traces/')\n",
    "        for n in range(n_samples):\n",
    "            channel,idx=self.channel_spindx[np.random.randint(len(self.channel_spindx))]\n",
    "            trace=self.filtdata[channel,idx-window_size:idx+window_size]\n",
    "            fig,ax=plt.subplots(1,1,figsize=(10,3))\n",
    "            x=1000*np.linspace(-window_size,window_size,2*window_size)/20000.\n",
    "            ax.plot(x,trace)\n",
    "            ax.plot(x[window_size:window_size+40],trace[window_size:window_size+40], color='r')\n",
    "            ax.set_xlabel('Time [ms]')\n",
    "            ax.set_ylabel('Voltage [uV]')\n",
    "            ax.set_title('Channel:'+str(channel)+', '+'Time stamp : '+str(idx))\n",
    "            plt.savefig(self.path+'/sample_traces/'+str(n)+'.svg',bbox_inches='tight',pad_inches=0.3)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_to_npy_data = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKy9j-4AhR1u",
    "outputId": "8cbacebc-e594-4fbc-c8f8-3e6a1cb8f8b3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment = Analyser(file_path_to_npy_data)\n",
    "experiment.get_peaks_and_get_waveforms()\n",
    "experiment.extract_shape_clusters(close_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, we extracted large events, first. We see that cluster 0 and 1 are noisy spikes. Cluster 2 is electrophysiological. So we take the good clusters, and we use them to template match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBDnyIDmhR1v"
   },
   "outputs": [],
   "source": [
    "experiment.match_cluster([2]) #take a list of good clusters like [0,1,2...] but here, only [2]\n",
    "experiment.save_spike_cutouts()\n",
    "experiment.produce_sample_detections(20,400) #how many examples to produce to verify the matching, and the length of each sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point, we have all the time stamps, we produced\n",
    "* some verification traces in data/sample_traces\n",
    "* We saved the shapes of the spikes in spike_shapes.pickle\n",
    "* and channel_spindx.npy saves the channel,timestamp pairs of peak detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the firing rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8ggWwg5hR1w"
   },
   "outputs": [],
   "source": [
    "chan_spindx=np.load(file_path_to_npy_data+'/channel_spindx.npy')\n",
    "#or, if it's still in memory: chan_spindx = experiment.channel_spindx\n",
    "spike_total=[np.sum(chan_spindx[:,0]==chan) for chan in range(np.max(chan_spindx[:,0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SWEAXVJhR1w"
   },
   "outputs": [],
   "source": [
    "file_duration = experiment.filtdata.shape[1]\n",
    "file_duration_in_s = file_duration/20000 #20 khz sampling\n",
    "fire_rate_hz = [f/file_duration_in_s for f in spike_total]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEMsOR49hR1w",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "width=5*20000 #seconds x sampling rate\n",
    "t=np.linspace(-width/2,width/2,width)\n",
    "gauss_kern=np.exp(-(t)**2/(0.1*width**2))\n",
    "gauss_kern/=np.sum(gauss_kern)\n",
    "\n",
    "density=[]\n",
    "resolution=int(file_duration_in_s)# how many pixels to show the density plot on\n",
    "for chan in range(len(fire_rate_hz)):\n",
    "    tmp=np.zeros(int(file_duration))\n",
    "    ch_sp=np.load(file_path_to_npy_data+'/channel_spindx.npy')\n",
    "    tmp[ch_sp[ch_sp[:,0]==chan,1]]=1 #trace with '1' where there's a spike\n",
    "    convolved_tmp=fftconvolve(tmp,gauss_kern,mode='same')\n",
    "    x=np.arange(len(convolved_tmp))\n",
    "    fy=interp1d(x,convolved_tmp)\n",
    "    density.append(fy(np.linspace(0,x[-1],resolution)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkXelDnHhR1-"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "mi=np.min(np.concatenate(density))\n",
    "low=mi+iqr(np.concatenate(density),rng=(0,0.5))\n",
    "high=mi+iqr(np.concatenate(density),rng=(0,99.5))\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import matplotlib.cm as cm\n",
    "bluz = cm.get_cmap('Blues', 1024)\n",
    "newcolors = bluz(np.linspace(0, 1, 1024))\n",
    "pink = np.array(newcolors[5])\n",
    "newcolors[10:50, :] = newcolors[50]\n",
    "newcolors[:10, :] = newcolors[0]\n",
    "newcmp = ListedColormap(newcolors)\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(8,4))\n",
    "im=ax.imshow(np.asarray(density)*(len(gauss_kern)/5),aspect='auto',interpolation=None,cmap=newcmp,vmin=0.1,vmax=high*(len(gauss_kern)/5))\n",
    "plt.colorbar(im)\n",
    "plt.ylabel('Channel index')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.title('Local firing rate [Hz]')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
